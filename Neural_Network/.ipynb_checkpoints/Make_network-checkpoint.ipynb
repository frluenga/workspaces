{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd51d11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.core.display import display,HTML\n",
    "\n",
    "from sklearn.datasets import make_circles\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d54dd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-056f7f3439b5>:16: MatplotlibDeprecationWarning: shading='flat' when X and Y have the same dimensions as C is deprecated since 3.3.  Either specify the corners of the quadrilaterals with X and Y, or pass shading='auto', 'nearest' or 'gouraud', or set rcParams['pcolor.shading'].  This will become an error two minor releases later.\n",
      "  plt.pcolormesh(_x0,_x1,_py,cmap='coolwarm',vmin=0,vmax=1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X,y = make_circles(n_samples=500,factor=0.5,noise=0.05)\n",
    "res = 100\n",
    "\n",
    "# Coordenadas del mapa de predicción\n",
    "_x0 = np.linspace(-1.5,1.5,res)\n",
    "_x1 = np.linspace(-1.5,1.5,res)\n",
    "\n",
    "# Input con cada combo de coordenadas de predicción\n",
    "_pX = np.array(np.meshgrid(_x0,_x1)).T.reshape(-1,2)\n",
    "\n",
    "# Objeto vacio a 0.5 del mapa de predicción\n",
    "_py = np.zeros((res,res))+0.5\n",
    "\n",
    "# visualización del mapa de predicción\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.pcolormesh(_x0,_x1,_py,cmap='coolwarm',vmin=0,vmax=1)\n",
    "\n",
    "# visualización de la nube de datos\n",
    "\n",
    "plt.scatter(X[y==0,0],X[y==0,1], c='skyblue')\n",
    "plt.scatter(X[y==1,0],X[y==1,1], c='salmon')\n",
    "\n",
    "plt.tick_params(labelbottom=False,labelleft=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4da95c5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ce9482d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (500,) for Tensor 'Placeholder_11:0', which has shape '(500, 1)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-676c56028c33>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;31m# Evaluamos al optimizador, a la función de coste y al tensor de salida pY.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;31m# La evaluación del optimizer producirá el entrenamiento de la red.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_pY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpY\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m \u001b[0miX\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miY\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;31m# Cada 25 iteraciones, imprimimos métricas.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dataAnalyst\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    955\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 957\u001b[1;33m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[0;32m    958\u001b[0m                          run_metadata_ptr)\n\u001b[0;32m    959\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dataAnalyst\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1152\u001b[0m           if (not is_tensor_handle_feed and\n\u001b[0;32m   1153\u001b[0m               not subfeed_t.get_shape().is_compatible_with(np_val.shape)):\n\u001b[1;32m-> 1154\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m   1155\u001b[0m                 \u001b[1;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m                 \u001b[1;34m'which has shape %r'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape (500,) for Tensor 'Placeholder_11:0', which has shape '(500, 1)'"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "# Definimos los puntos de entrada de la red, para la matriz X e Y.\n",
    "iX = tf.compat.v1.placeholder('float', shape=[500,2]) # ERROR LIBRERIA\n",
    "iY = tf.compat.v1.placeholder('float', shape=[500,1])\n",
    "\n",
    "lr = 0.01           # learning rate\n",
    "nn = [2, 16, 8, 1]  # número de neuronas por capa.\n",
    "\n",
    "# Capa 1\n",
    "W1 = tf.Variable(tf.random.normal([nn[0], nn[1]]), name='Weights_1')\n",
    "b1 = tf.Variable(tf.random.normal([nn[1]]), name='bias_1')\n",
    "\n",
    "l1 = tf.nn.relu(tf.add(tf.matmul(iX, W1), b1))\n",
    "\n",
    "# Capa 2\n",
    "W2 = tf.Variable(tf.random.normal([nn[1], nn[2]]), name='Weights_2')\n",
    "b2 = tf.Variable(tf.random.normal([nn[2]]), name='bias_2')\n",
    "\n",
    "l2 = tf.nn.relu(tf.add(tf.matmul(l1, W2), b2))\n",
    "\n",
    "# Capa 3\n",
    "W3 = tf.Variable(tf.random.normal([nn[2], nn[3]]), name='Weights_3')\n",
    "b3 = tf.Variable(tf.random.normal([nn[3]]), name='bias_3')\n",
    "\n",
    "# Vector de predicciones de Y.\n",
    "pY = tf.nn.sigmoid(tf.add(tf.matmul(l2, W3), b3))[:, 0]\n",
    "\n",
    "\n",
    "# Evaluación de las predicciones.\n",
    "loss = tf.losses.mean_squared_error(pY, iY)\n",
    "\n",
    "# Definimos al optimizador de la red, para que minimice el error.\n",
    "optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.05).minimize(loss)\n",
    "\n",
    "n_steps = 1000 # Número de ciclos de entrenamiento.\n",
    "\n",
    "iPY = [] # Aquí guardaremos la evolución de las predicción, para la animación.\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "  \n",
    "  # Inicializamos todos los parámetros de la red, las matrices W y b.\n",
    "  sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    \n",
    "  # Iteramos n pases de entrenamiento.\n",
    "  for step in range(n_steps):\n",
    "  \n",
    "    # Evaluamos al optimizador, a la función de coste y al tensor de salida pY. \n",
    "    # La evaluación del optimizer producirá el entrenamiento de la red.\n",
    "    _, _loss, _pY = sess.run([optimizer, loss, pY], feed_dict={ iX : X, iY : y })\n",
    "    \n",
    "    # Cada 25 iteraciones, imprimimos métricas.\n",
    "    if step % 25 == 0: \n",
    "      \n",
    "      # Cálculo del accuracy.\n",
    "      acc = np.mean(np.round(_pY) == y)\n",
    "      \n",
    "      # Impresión de métricas.\n",
    "      print('Step', step, '/', n_steps, '- Loss = ', _loss, '- Acc =', acc)\n",
    "      \n",
    "      # Obtenemos predicciones para cada punto de nuestro mapa de predicción _pX.\n",
    "      _pY = sess.run(pY, feed_dict={ iX : _pX }).reshape((res, res))\n",
    "\n",
    "      # Y lo guardamos para visualizar la animación.\n",
    "      iPY.append(_pY)\n",
    "      \n",
    "  \n",
    "# ----- CÓDIGO ANIMACIÓN ----- #\n",
    "\n",
    "ims = []\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "print(\"--- Generando animación ---\")\n",
    "\n",
    "for fr in range(len(iPY)):\n",
    "  \n",
    "  im = plt.pcolormesh(_x0, _x1, iPY[fr], cmap=\"coolwarm\", animated=True)\n",
    "\n",
    "  # Visualización de la nube de datos.\n",
    "  plt.scatter(X[y == 0,0], X[y == 0,1], c=\"skyblue\")\n",
    "  plt.scatter(X[y == 1,0], X[y == 1,1], c=\"salmon\")\n",
    "\n",
    "  # plt.title(\"Resultado Clasificación\")\n",
    "  plt.tick_params(labelbottom=False, labelleft=False)\n",
    "\n",
    "  ims.append([im])\n",
    "\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=50, blit=True, repeat_delay=1000)\n",
    "\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b21c9623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras as kr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cfb45c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate\n",
    "lr = 0.01\n",
    "# Cantidad de neuronas por capa\n",
    "nn = [2,16,8,1]\n",
    "# Creamos la estructura que contendra el modelo (Secuencia de capas)\n",
    "model = kr.Sequential()\n",
    "\n",
    "# Las capas fully connected es decir cada neurona tiene una conexión con\n",
    "# la capa anterior son las capas dense\n",
    "# Especifica cuantas neuronas tiene y la función de activación\n",
    "\n",
    "# Capa 1\n",
    "model.add(kr.layers.Dense(nn[1],activation='relu'))\n",
    "\n",
    "# Capa 2\n",
    "model.add(kr.layers.Dense(nn[2],activation='relu'))\n",
    "\n",
    "# Capa 3\n",
    "model.add(kr.layers.Dense(nn[3],activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5aa63c30",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 500 samples\n",
      "Epoch 1/250\n",
      "500/500 [==============================] - 0s 32us/sample - loss: 0.2621 - acc: 0.4920\n",
      "Epoch 2/250\n",
      "500/500 [==============================] - 0s 34us/sample - loss: 0.2613 - acc: 0.4860\n",
      "Epoch 3/250\n",
      "500/500 [==============================] - 0s 32us/sample - loss: 0.2604 - acc: 0.4840\n",
      "Epoch 4/250\n",
      "500/500 [==============================] - 0s 32us/sample - loss: 0.2597 - acc: 0.4760\n",
      "Epoch 5/250\n",
      "500/500 [==============================] - 0s 38us/sample - loss: 0.2590 - acc: 0.4760\n",
      "Epoch 6/250\n",
      "500/500 [==============================] - 0s 30us/sample - loss: 0.2583 - acc: 0.4780\n",
      "Epoch 7/250\n",
      "500/500 [==============================] - 0s 34us/sample - loss: 0.2576 - acc: 0.4740\n",
      "Epoch 8/250\n",
      "500/500 [==============================] - 0s 32us/sample - loss: 0.2570 - acc: 0.4700\n",
      "Epoch 9/250\n",
      "500/500 [==============================] - 0s 30us/sample - loss: 0.2564 - acc: 0.4740\n",
      "Epoch 10/250\n",
      "500/500 [==============================] - 0s 30us/sample - loss: 0.2558 - acc: 0.4760\n",
      "Epoch 11/250\n",
      "500/500 [==============================] - 0s 30us/sample - loss: 0.2552 - acc: 0.4880\n",
      "Epoch 12/250\n",
      "500/500 [==============================] - 0s 28us/sample - loss: 0.2547 - acc: 0.4820\n",
      "Epoch 13/250\n",
      "500/500 [==============================] - 0s 32us/sample - loss: 0.2541 - acc: 0.4800\n",
      "Epoch 14/250\n",
      "500/500 [==============================] - 0s 32us/sample - loss: 0.2536 - acc: 0.4860\n",
      "Epoch 15/250\n",
      "500/500 [==============================] - 0s 32us/sample - loss: 0.2531 - acc: 0.4880\n",
      "Epoch 16/250\n",
      "500/500 [==============================] - 0s 32us/sample - loss: 0.2525 - acc: 0.4840\n",
      "Epoch 17/250\n",
      "500/500 [==============================] - 0s 30us/sample - loss: 0.2520 - acc: 0.4840\n",
      "Epoch 18/250\n",
      "500/500 [==============================] - 0s 30us/sample - loss: 0.2515 - acc: 0.4820\n",
      "Epoch 19/250\n",
      "500/500 [==============================] - 0s 28us/sample - loss: 0.2510 - acc: 0.4840\n",
      "Epoch 20/250\n",
      "500/500 [==============================] - 0s 28us/sample - loss: 0.2506 - acc: 0.4780\n",
      "Epoch 21/250\n",
      "500/500 [==============================] - 0s 28us/sample - loss: 0.2501 - acc: 0.4780\n",
      "Epoch 22/250\n",
      "500/500 [==============================] - 0s 28us/sample - loss: 0.2496 - acc: 0.4860\n",
      "Epoch 23/250\n",
      "500/500 [==============================] - 0s 26us/sample - loss: 0.2492 - acc: 0.4740\n",
      "Epoch 24/250\n",
      "500/500 [==============================] - 0s 28us/sample - loss: 0.2487 - acc: 0.4740\n",
      "Epoch 25/250\n",
      "500/500 [==============================] - 0s 28us/sample - loss: 0.2483 - acc: 0.4780\n",
      "Epoch 26/250\n",
      "500/500 [==============================] - 0s 26us/sample - loss: 0.2478 - acc: 0.4820\n",
      "Epoch 27/250\n",
      "500/500 [==============================] - 0s 28us/sample - loss: 0.2474 - acc: 0.4820\n",
      "Epoch 28/250\n",
      "500/500 [==============================] - 0s 30us/sample - loss: 0.2470 - acc: 0.4840\n",
      "Epoch 29/250\n",
      "500/500 [==============================] - 0s 30us/sample - loss: 0.2466 - acc: 0.4840\n",
      "Epoch 30/250\n",
      "500/500 [==============================] - 0s 22us/sample - loss: 0.2462 - acc: 0.4820\n",
      "Epoch 31/250\n",
      "500/500 [==============================] - 0s 30us/sample - loss: 0.2458 - acc: 0.4900\n",
      "Epoch 32/250\n",
      "500/500 [==============================] - 0s 30us/sample - loss: 0.2454 - acc: 0.4700\n",
      "Epoch 33/250\n",
      "500/500 [==============================] - 0s 28us/sample - loss: 0.2450 - acc: 0.4740\n",
      "Epoch 34/250\n",
      "500/500 [==============================] - 0s 28us/sample - loss: 0.2446 - acc: 0.4820\n",
      "Epoch 35/250\n",
      "500/500 [==============================] - 0s 28us/sample - loss: 0.2442 - acc: 0.4920\n",
      "Epoch 36/250\n",
      "500/500 [==============================] - 0s 28us/sample - loss: 0.2438 - acc: 0.4600\n",
      "Epoch 37/250\n",
      "500/500 [==============================] - 0s 32us/sample - loss: 0.2434 - acc: 0.4820\n",
      "Epoch 38/250\n",
      "500/500 [==============================] - 0s 26us/sample - loss: 0.2430 - acc: 0.4500\n",
      "Epoch 39/250\n",
      "500/500 [==============================] - 0s 32us/sample - loss: 0.2426 - acc: 0.4440\n",
      "Epoch 40/250\n",
      "500/500 [==============================] - 0s 28us/sample - loss: 0.2422 - acc: 0.4880\n",
      "Epoch 41/250\n",
      "500/500 [==============================] - 0s 26us/sample - loss: 0.2418 - acc: 0.5020\n",
      "Epoch 42/250\n",
      "500/500 [==============================] - ETA: 0s - loss: 0.2436 - acc: 0.468 - 0s 26us/sample - loss: 0.2414 - acc: 0.5140\n",
      "Epoch 43/250\n",
      "500/500 [==============================] - 0s 28us/sample - loss: 0.2410 - acc: 0.5160\n",
      "Epoch 44/250\n",
      "500/500 [==============================] - 0s 30us/sample - loss: 0.2406 - acc: 0.5220\n",
      "Epoch 45/250\n",
      "500/500 [==============================] - 0s 34us/sample - loss: 0.2402 - acc: 0.5260\n",
      "Epoch 46/250\n",
      "500/500 [==============================] - 0s 36us/sample - loss: 0.2398 - acc: 0.5300\n",
      "Epoch 47/250\n",
      "500/500 [==============================] - 0s 30us/sample - loss: 0.2394 - acc: 0.5240\n",
      "Epoch 48/250\n",
      "500/500 [==============================] - 0s 32us/sample - loss: 0.2390 - acc: 0.5360\n",
      "Epoch 49/250\n",
      "500/500 [==============================] - 0s 28us/sample - loss: 0.2385 - acc: 0.5340\n",
      "Epoch 50/250\n",
      "500/500 [==============================] - 0s 30us/sample - loss: 0.2380 - acc: 0.5500\n",
      "Epoch 51/250\n",
      "500/500 [==============================] - 0s 26us/sample - loss: 0.2376 - acc: 0.5540\n",
      "Epoch 52/250\n",
      "500/500 [==============================] - 0s 30us/sample - loss: 0.2372 - acc: 0.5620\n",
      "Epoch 53/250\n",
      "500/500 [==============================] - 0s 30us/sample - loss: 0.2368 - acc: 0.5620\n",
      "Epoch 54/250\n",
      "500/500 [==============================] - 0s 30us/sample - loss: 0.2365 - acc: 0.5620\n",
      "Epoch 55/250\n",
      "500/500 [==============================] - 0s 28us/sample - loss: 0.2361 - acc: 0.5700\n",
      "Epoch 56/250\n",
      "500/500 [==============================] - 0s 30us/sample - loss: 0.2357 - acc: 0.5700\n",
      "Epoch 57/250\n",
      "500/500 [==============================] - 0s 26us/sample - loss: 0.2353 - acc: 0.5740\n",
      "Epoch 58/250\n",
      "500/500 [==============================] - 0s 30us/sample - loss: 0.2349 - acc: 0.5840\n",
      "Epoch 59/250\n",
      "500/500 [==============================] - 0s 28us/sample - loss: 0.2346 - acc: 0.5840\n",
      "Epoch 60/250\n",
      "500/500 [==============================] - 0s 28us/sample - loss: 0.2342 - acc: 0.6000\n",
      "Epoch 61/250\n",
      "500/500 [==============================] - 0s 26us/sample - loss: 0.2339 - acc: 0.6080\n",
      "Epoch 62/250\n",
      "500/500 [==============================] - 0s 26us/sample - loss: 0.2335 - acc: 0.6080\n",
      "Epoch 63/250\n",
      "500/500 [==============================] - 0s 28us/sample - loss: 0.2331 - acc: 0.6180\n",
      "Epoch 64/250\n",
      "500/500 [==============================] - 0s 28us/sample - loss: 0.2328 - acc: 0.6180\n",
      "Epoch 65/250\n",
      "500/500 [==============================] - 0s 26us/sample - loss: 0.2324 - acc: 0.6300\n",
      "Epoch 66/250\n",
      "500/500 [==============================] - 0s 28us/sample - loss: 0.2321 - acc: 0.6320\n",
      "Epoch 67/250\n",
      "500/500 [==============================] - 0s 30us/sample - loss: 0.2317 - acc: 0.6360\n",
      "Epoch 68/250\n",
      "500/500 [==============================] - 0s 28us/sample - loss: 0.2314 - acc: 0.6380\n",
      "Epoch 69/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.2310 - acc: 0.6400\n",
      "Epoch 70/250\n",
      "500/500 [==============================] - 0s 30us/sample - loss: 0.2306 - acc: 0.6400\n",
      "Epoch 71/250\n",
      "500/500 [==============================] - 0s 26us/sample - loss: 0.2303 - acc: 0.6420\n",
      "Epoch 72/250\n",
      "500/500 [==============================] - 0s 28us/sample - loss: 0.2299 - acc: 0.6400\n",
      "Epoch 73/250\n",
      "500/500 [==============================] - 0s 26us/sample - loss: 0.2295 - acc: 0.6480\n",
      "Epoch 74/250\n",
      "500/500 [==============================] - 0s 28us/sample - loss: 0.2291 - acc: 0.6600\n",
      "Epoch 75/250\n",
      "500/500 [==============================] - 0s 32us/sample - loss: 0.2287 - acc: 0.6600\n",
      "Epoch 76/250\n",
      "500/500 [==============================] - 0s 28us/sample - loss: 0.2282 - acc: 0.6580\n",
      "Epoch 77/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.2278 - acc: 0.6680\n",
      "Epoch 78/250\n",
      "500/500 [==============================] - 0s 28us/sample - loss: 0.2274 - acc: 0.6740\n",
      "Epoch 79/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.2269 - acc: 0.6800\n",
      "Epoch 80/250\n",
      "500/500 [==============================] - 0s 26us/sample - loss: 0.2265 - acc: 0.6820\n",
      "Epoch 81/250\n",
      "500/500 [==============================] - 0s 26us/sample - loss: 0.2261 - acc: 0.6820\n",
      "Epoch 82/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s 24us/sample - loss: 0.2256 - acc: 0.6900\n",
      "Epoch 83/250\n",
      "500/500 [==============================] - 0s 30us/sample - loss: 0.2252 - acc: 0.6920\n",
      "Epoch 84/250\n",
      "500/500 [==============================] - 0s 28us/sample - loss: 0.2248 - acc: 0.6880\n",
      "Epoch 85/250\n",
      "500/500 [==============================] - 0s 28us/sample - loss: 0.2244 - acc: 0.6940\n",
      "Epoch 86/250\n",
      "500/500 [==============================] - 0s 30us/sample - loss: 0.2240 - acc: 0.6900\n",
      "Epoch 87/250\n",
      "500/500 [==============================] - 0s 26us/sample - loss: 0.2236 - acc: 0.6940\n",
      "Epoch 88/250\n",
      "500/500 [==============================] - 0s 28us/sample - loss: 0.2233 - acc: 0.6940\n",
      "Epoch 89/250\n",
      "500/500 [==============================] - 0s 28us/sample - loss: 0.2229 - acc: 0.6900\n",
      "Epoch 90/250\n",
      "500/500 [==============================] - 0s 26us/sample - loss: 0.2225 - acc: 0.6980\n",
      "Epoch 91/250\n",
      "500/500 [==============================] - 0s 26us/sample - loss: 0.2221 - acc: 0.7000\n",
      "Epoch 92/250\n",
      "500/500 [==============================] - 0s 28us/sample - loss: 0.2217 - acc: 0.6980\n",
      "Epoch 93/250\n",
      "500/500 [==============================] - 0s 30us/sample - loss: 0.2213 - acc: 0.7080\n",
      "Epoch 94/250\n",
      "500/500 [==============================] - 0s 28us/sample - loss: 0.2209 - acc: 0.7100\n",
      "Epoch 95/250\n",
      "500/500 [==============================] - 0s 30us/sample - loss: 0.2204 - acc: 0.7180\n",
      "Epoch 96/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.2200 - acc: 0.7320\n",
      "Epoch 97/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.2196 - acc: 0.7280\n",
      "Epoch 98/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.2192 - acc: 0.7320\n",
      "Epoch 99/250\n",
      "500/500 [==============================] - 0s 22us/sample - loss: 0.2188 - acc: 0.7360\n",
      "Epoch 100/250\n",
      "500/500 [==============================] - 0s 22us/sample - loss: 0.2184 - acc: 0.7340\n",
      "Epoch 101/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.2179 - acc: 0.7480\n",
      "Epoch 102/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.2175 - acc: 0.7560\n",
      "Epoch 103/250\n",
      "500/500 [==============================] - 0s 28us/sample - loss: 0.2171 - acc: 0.7520\n",
      "Epoch 104/250\n",
      "500/500 [==============================] - 0s 28us/sample - loss: 0.2167 - acc: 0.7560\n",
      "Epoch 105/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.2162 - acc: 0.7580\n",
      "Epoch 106/250\n",
      "500/500 [==============================] - 0s 22us/sample - loss: 0.2158 - acc: 0.7620\n",
      "Epoch 107/250\n",
      "500/500 [==============================] - 0s 22us/sample - loss: 0.2153 - acc: 0.7680\n",
      "Epoch 108/250\n",
      "500/500 [==============================] - 0s 22us/sample - loss: 0.2149 - acc: 0.7680\n",
      "Epoch 109/250\n",
      "500/500 [==============================] - 0s 17us/sample - loss: 0.2145 - acc: 0.7680\n",
      "Epoch 110/250\n",
      "500/500 [==============================] - 0s 20us/sample - loss: 0.2141 - acc: 0.7720\n",
      "Epoch 111/250\n",
      "500/500 [==============================] - 0s 22us/sample - loss: 0.2136 - acc: 0.7720\n",
      "Epoch 112/250\n",
      "500/500 [==============================] - 0s 22us/sample - loss: 0.2132 - acc: 0.7780\n",
      "Epoch 113/250\n",
      "500/500 [==============================] - 0s 20us/sample - loss: 0.2127 - acc: 0.7800\n",
      "Epoch 114/250\n",
      "500/500 [==============================] - 0s 19us/sample - loss: 0.2123 - acc: 0.7780\n",
      "Epoch 115/250\n",
      "500/500 [==============================] - 0s 22us/sample - loss: 0.2119 - acc: 0.7840\n",
      "Epoch 116/250\n",
      "500/500 [==============================] - 0s 22us/sample - loss: 0.2114 - acc: 0.7820\n",
      "Epoch 117/250\n",
      "500/500 [==============================] - 0s 26us/sample - loss: 0.2110 - acc: 0.7840\n",
      "Epoch 118/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.2105 - acc: 0.7900\n",
      "Epoch 119/250\n",
      "500/500 [==============================] - 0s 26us/sample - loss: 0.2100 - acc: 0.7920\n",
      "Epoch 120/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.2096 - acc: 0.7920\n",
      "Epoch 121/250\n",
      "500/500 [==============================] - 0s 26us/sample - loss: 0.2091 - acc: 0.8000\n",
      "Epoch 122/250\n",
      "500/500 [==============================] - 0s 26us/sample - loss: 0.2086 - acc: 0.8000\n",
      "Epoch 123/250\n",
      "500/500 [==============================] - 0s 26us/sample - loss: 0.2082 - acc: 0.8020\n",
      "Epoch 124/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.2077 - acc: 0.7980\n",
      "Epoch 125/250\n",
      "500/500 [==============================] - 0s 26us/sample - loss: 0.2073 - acc: 0.8040\n",
      "Epoch 126/250\n",
      "500/500 [==============================] - 0s 28us/sample - loss: 0.2068 - acc: 0.8020\n",
      "Epoch 127/250\n",
      "500/500 [==============================] - 0s 28us/sample - loss: 0.2063 - acc: 0.8080\n",
      "Epoch 128/250\n",
      "500/500 [==============================] - 0s 26us/sample - loss: 0.2058 - acc: 0.8120\n",
      "Epoch 129/250\n",
      "500/500 [==============================] - 0s 26us/sample - loss: 0.2053 - acc: 0.8160\n",
      "Epoch 130/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.2049 - acc: 0.8120\n",
      "Epoch 131/250\n",
      "500/500 [==============================] - 0s 26us/sample - loss: 0.2044 - acc: 0.8220\n",
      "Epoch 132/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.2038 - acc: 0.8180\n",
      "Epoch 133/250\n",
      "500/500 [==============================] - 0s 26us/sample - loss: 0.2033 - acc: 0.8220\n",
      "Epoch 134/250\n",
      "500/500 [==============================] - 0s 28us/sample - loss: 0.2028 - acc: 0.8160\n",
      "Epoch 135/250\n",
      "500/500 [==============================] - 0s 28us/sample - loss: 0.2023 - acc: 0.8280\n",
      "Epoch 136/250\n",
      "500/500 [==============================] - 0s 30us/sample - loss: 0.2017 - acc: 0.8320\n",
      "Epoch 137/250\n",
      "500/500 [==============================] - 0s 28us/sample - loss: 0.2010 - acc: 0.8400\n",
      "Epoch 138/250\n",
      "500/500 [==============================] - 0s 26us/sample - loss: 0.2003 - acc: 0.8340\n",
      "Epoch 139/250\n",
      "500/500 [==============================] - 0s 26us/sample - loss: 0.1995 - acc: 0.8460\n",
      "Epoch 140/250\n",
      "500/500 [==============================] - 0s 34us/sample - loss: 0.1987 - acc: 0.8460\n",
      "Epoch 141/250\n",
      "500/500 [==============================] - 0s 32us/sample - loss: 0.1980 - acc: 0.8520\n",
      "Epoch 142/250\n",
      "500/500 [==============================] - 0s 30us/sample - loss: 0.1972 - acc: 0.8600\n",
      "Epoch 143/250\n",
      "500/500 [==============================] - 0s 36us/sample - loss: 0.1965 - acc: 0.8580\n",
      "Epoch 144/250\n",
      "500/500 [==============================] - 0s 26us/sample - loss: 0.1958 - acc: 0.8580\n",
      "Epoch 145/250\n",
      "500/500 [==============================] - 0s 28us/sample - loss: 0.1951 - acc: 0.8600\n",
      "Epoch 146/250\n",
      "500/500 [==============================] - 0s 28us/sample - loss: 0.1944 - acc: 0.8580\n",
      "Epoch 147/250\n",
      "500/500 [==============================] - 0s 28us/sample - loss: 0.1937 - acc: 0.8600\n",
      "Epoch 148/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.1931 - acc: 0.8600\n",
      "Epoch 149/250\n",
      "500/500 [==============================] - 0s 20us/sample - loss: 0.1925 - acc: 0.8620\n",
      "Epoch 150/250\n",
      "500/500 [==============================] - 0s 22us/sample - loss: 0.1919 - acc: 0.8600\n",
      "Epoch 151/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.1912 - acc: 0.8620\n",
      "Epoch 152/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.1907 - acc: 0.8640\n",
      "Epoch 153/250\n",
      "500/500 [==============================] - 0s 22us/sample - loss: 0.1901 - acc: 0.8640\n",
      "Epoch 154/250\n",
      "500/500 [==============================] - 0s 22us/sample - loss: 0.1895 - acc: 0.8680\n",
      "Epoch 155/250\n",
      "500/500 [==============================] - 0s 26us/sample - loss: 0.1890 - acc: 0.8660\n",
      "Epoch 156/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.1884 - acc: 0.8660\n",
      "Epoch 157/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.1878 - acc: 0.8700\n",
      "Epoch 158/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.1872 - acc: 0.8660\n",
      "Epoch 159/250\n",
      "500/500 [==============================] - 0s 22us/sample - loss: 0.1867 - acc: 0.8640\n",
      "Epoch 160/250\n",
      "500/500 [==============================] - 0s 22us/sample - loss: 0.1861 - acc: 0.8660\n",
      "Epoch 161/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.1855 - acc: 0.8680\n",
      "Epoch 162/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.1850 - acc: 0.8680\n",
      "Epoch 163/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s 20us/sample - loss: 0.1844 - acc: 0.8700\n",
      "Epoch 164/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.1838 - acc: 0.8700\n",
      "Epoch 165/250\n",
      "500/500 [==============================] - 0s 20us/sample - loss: 0.1832 - acc: 0.8700\n",
      "Epoch 166/250\n",
      "500/500 [==============================] - 0s 22us/sample - loss: 0.1827 - acc: 0.8700\n",
      "Epoch 167/250\n",
      "500/500 [==============================] - 0s 22us/sample - loss: 0.1821 - acc: 0.8680\n",
      "Epoch 168/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.1815 - acc: 0.8720\n",
      "Epoch 169/250\n",
      "500/500 [==============================] - 0s 22us/sample - loss: 0.1809 - acc: 0.8700\n",
      "Epoch 170/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.1804 - acc: 0.8700\n",
      "Epoch 171/250\n",
      "500/500 [==============================] - 0s 20us/sample - loss: 0.1798 - acc: 0.8740\n",
      "Epoch 172/250\n",
      "500/500 [==============================] - 0s 18us/sample - loss: 0.1792 - acc: 0.8720\n",
      "Epoch 173/250\n",
      "500/500 [==============================] - 0s 20us/sample - loss: 0.1786 - acc: 0.8740\n",
      "Epoch 174/250\n",
      "500/500 [==============================] - 0s 22us/sample - loss: 0.1780 - acc: 0.8740\n",
      "Epoch 175/250\n",
      "500/500 [==============================] - 0s 26us/sample - loss: 0.1774 - acc: 0.8760\n",
      "Epoch 176/250\n",
      "500/500 [==============================] - 0s 22us/sample - loss: 0.1768 - acc: 0.8760\n",
      "Epoch 177/250\n",
      "500/500 [==============================] - 0s 22us/sample - loss: 0.1762 - acc: 0.8760\n",
      "Epoch 178/250\n",
      "500/500 [==============================] - 0s 18us/sample - loss: 0.1757 - acc: 0.8760\n",
      "Epoch 179/250\n",
      "500/500 [==============================] - 0s 22us/sample - loss: 0.1751 - acc: 0.8760\n",
      "Epoch 180/250\n",
      "500/500 [==============================] - 0s 22us/sample - loss: 0.1745 - acc: 0.8760\n",
      "Epoch 181/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.1739 - acc: 0.8780\n",
      "Epoch 182/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.1733 - acc: 0.8780\n",
      "Epoch 183/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.1727 - acc: 0.8800\n",
      "Epoch 184/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.1721 - acc: 0.8800\n",
      "Epoch 185/250\n",
      "500/500 [==============================] - 0s 22us/sample - loss: 0.1715 - acc: 0.8800\n",
      "Epoch 186/250\n",
      "500/500 [==============================] - 0s 20us/sample - loss: 0.1709 - acc: 0.8820\n",
      "Epoch 187/250\n",
      "500/500 [==============================] - 0s 20us/sample - loss: 0.1702 - acc: 0.8820\n",
      "Epoch 188/250\n",
      "500/500 [==============================] - 0s 22us/sample - loss: 0.1697 - acc: 0.8840\n",
      "Epoch 189/250\n",
      "500/500 [==============================] - 0s 26us/sample - loss: 0.1690 - acc: 0.8820\n",
      "Epoch 190/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.1684 - acc: 0.8820\n",
      "Epoch 191/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.1678 - acc: 0.8860\n",
      "Epoch 192/250\n",
      "500/500 [==============================] - 0s 22us/sample - loss: 0.1672 - acc: 0.8880\n",
      "Epoch 193/250\n",
      "500/500 [==============================] - 0s 22us/sample - loss: 0.1666 - acc: 0.8880\n",
      "Epoch 194/250\n",
      "500/500 [==============================] - 0s 20us/sample - loss: 0.1659 - acc: 0.8860\n",
      "Epoch 195/250\n",
      "500/500 [==============================] - 0s 20us/sample - loss: 0.1653 - acc: 0.8860\n",
      "Epoch 196/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.1647 - acc: 0.8920\n",
      "Epoch 197/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.1640 - acc: 0.8880\n",
      "Epoch 198/250\n",
      "500/500 [==============================] - 0s 22us/sample - loss: 0.1634 - acc: 0.8880\n",
      "Epoch 199/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.1628 - acc: 0.8940\n",
      "Epoch 200/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.1621 - acc: 0.8900\n",
      "Epoch 201/250\n",
      "500/500 [==============================] - 0s 48us/sample - loss: 0.1615 - acc: 0.8900\n",
      "Epoch 202/250\n",
      "500/500 [==============================] - 0s 42us/sample - loss: 0.1608 - acc: 0.8940\n",
      "Epoch 203/250\n",
      "500/500 [==============================] - 0s 52us/sample - loss: 0.1602 - acc: 0.8920\n",
      "Epoch 204/250\n",
      "500/500 [==============================] - 0s 46us/sample - loss: 0.1595 - acc: 0.8920\n",
      "Epoch 205/250\n",
      "500/500 [==============================] - 0s 30us/sample - loss: 0.1589 - acc: 0.8980\n",
      "Epoch 206/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.1583 - acc: 0.8940\n",
      "Epoch 207/250\n",
      "500/500 [==============================] - 0s 22us/sample - loss: 0.1576 - acc: 0.8960\n",
      "Epoch 208/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.1569 - acc: 0.8960\n",
      "Epoch 209/250\n",
      "500/500 [==============================] - 0s 22us/sample - loss: 0.1563 - acc: 0.8940\n",
      "Epoch 210/250\n",
      "500/500 [==============================] - 0s 20us/sample - loss: 0.1556 - acc: 0.8960\n",
      "Epoch 211/250\n",
      "500/500 [==============================] - 0s 22us/sample - loss: 0.1549 - acc: 0.8960\n",
      "Epoch 212/250\n",
      "500/500 [==============================] - 0s 20us/sample - loss: 0.1542 - acc: 0.9020\n",
      "Epoch 213/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.1536 - acc: 0.9000\n",
      "Epoch 214/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.1529 - acc: 0.8960\n",
      "Epoch 215/250\n",
      "500/500 [==============================] - 0s 28us/sample - loss: 0.1522 - acc: 0.9000\n",
      "Epoch 216/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.1515 - acc: 0.9020\n",
      "Epoch 217/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.1508 - acc: 0.9060\n",
      "Epoch 218/250\n",
      "500/500 [==============================] - 0s 22us/sample - loss: 0.1501 - acc: 0.9100\n",
      "Epoch 219/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.1494 - acc: 0.9080\n",
      "Epoch 220/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.1487 - acc: 0.9080\n",
      "Epoch 221/250\n",
      "500/500 [==============================] - 0s 18us/sample - loss: 0.1480 - acc: 0.9100\n",
      "Epoch 222/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.1473 - acc: 0.9100\n",
      "Epoch 223/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.1466 - acc: 0.9100\n",
      "Epoch 224/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.1459 - acc: 0.9080\n",
      "Epoch 225/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.1451 - acc: 0.9120\n",
      "Epoch 226/250\n",
      "500/500 [==============================] - 0s 22us/sample - loss: 0.1444 - acc: 0.9120\n",
      "Epoch 227/250\n",
      "500/500 [==============================] - 0s 20us/sample - loss: 0.1437 - acc: 0.9120\n",
      "Epoch 228/250\n",
      "500/500 [==============================] - 0s 20us/sample - loss: 0.1430 - acc: 0.9140\n",
      "Epoch 229/250\n",
      "500/500 [==============================] - 0s 20us/sample - loss: 0.1422 - acc: 0.9140\n",
      "Epoch 230/250\n",
      "500/500 [==============================] - 0s 22us/sample - loss: 0.1415 - acc: 0.9140\n",
      "Epoch 231/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.1408 - acc: 0.9140\n",
      "Epoch 232/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.1400 - acc: 0.9160\n",
      "Epoch 233/250\n",
      "500/500 [==============================] - 0s 22us/sample - loss: 0.1393 - acc: 0.9160\n",
      "Epoch 234/250\n",
      "500/500 [==============================] - 0s 26us/sample - loss: 0.1386 - acc: 0.9160\n",
      "Epoch 235/250\n",
      "500/500 [==============================] - 0s 22us/sample - loss: 0.1378 - acc: 0.9200\n",
      "Epoch 236/250\n",
      "500/500 [==============================] - 0s 18us/sample - loss: 0.1371 - acc: 0.9200\n",
      "Epoch 237/250\n",
      "500/500 [==============================] - 0s 20us/sample - loss: 0.1363 - acc: 0.9200\n",
      "Epoch 238/250\n",
      "500/500 [==============================] - 0s 22us/sample - loss: 0.1356 - acc: 0.9200\n",
      "Epoch 239/250\n",
      "500/500 [==============================] - 0s 22us/sample - loss: 0.1348 - acc: 0.9200\n",
      "Epoch 240/250\n",
      "500/500 [==============================] - 0s 22us/sample - loss: 0.1341 - acc: 0.9220\n",
      "Epoch 241/250\n",
      "500/500 [==============================] - 0s 20us/sample - loss: 0.1333 - acc: 0.9220\n",
      "Epoch 242/250\n",
      "500/500 [==============================] - 0s 18us/sample - loss: 0.1325 - acc: 0.9260\n",
      "Epoch 243/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.1318 - acc: 0.9260\n",
      "Epoch 244/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s 24us/sample - loss: 0.1310 - acc: 0.9260\n",
      "Epoch 245/250\n",
      "500/500 [==============================] - 0s 22us/sample - loss: 0.1302 - acc: 0.9260\n",
      "Epoch 246/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.1295 - acc: 0.9260\n",
      "Epoch 247/250\n",
      "500/500 [==============================] - 0s 22us/sample - loss: 0.1287 - acc: 0.9300\n",
      "Epoch 248/250\n",
      "500/500 [==============================] - 0s 18us/sample - loss: 0.1279 - acc: 0.9320\n",
      "Epoch 249/250\n",
      "500/500 [==============================] - 0s 20us/sample - loss: 0.1271 - acc: 0.9340\n",
      "Epoch 250/250\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.1264 - acc: 0.9360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1fe91d59e50>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compilar el modelo\n",
    "model.compile(loss='mse',optimizer=kr.optimizers.SGD(lr=lr),metrics=['acc'])\n",
    "# Entrenamos el modelo\n",
    "model.fit(X,y,epochs=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "607ce2de",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.11727375\n",
      "Iteration 2, loss = 0.11471359\n",
      "Iteration 3, loss = 0.11366473\n",
      "Iteration 4, loss = 0.11350769\n",
      "Iteration 5, loss = 0.11289917\n",
      "Iteration 6, loss = 0.11218308\n",
      "Iteration 7, loss = 0.11174366\n",
      "Iteration 8, loss = 0.11151826\n",
      "Iteration 9, loss = 0.11119335\n",
      "Iteration 10, loss = 0.11090988\n",
      "Iteration 11, loss = 0.11064010\n",
      "Iteration 12, loss = 0.11036638\n",
      "Iteration 13, loss = 0.11008500\n",
      "Iteration 14, loss = 0.10981310\n",
      "Iteration 15, loss = 0.10951827\n",
      "Iteration 16, loss = 0.10926085\n",
      "Iteration 17, loss = 0.10896523\n",
      "Iteration 18, loss = 0.10872172\n",
      "Iteration 19, loss = 0.10836639\n",
      "Iteration 20, loss = 0.10810402\n",
      "Iteration 21, loss = 0.10779290\n",
      "Iteration 22, loss = 0.10753148\n",
      "Iteration 23, loss = 0.10721913\n",
      "Iteration 24, loss = 0.10692843\n",
      "Iteration 25, loss = 0.10665264\n",
      "Iteration 26, loss = 0.10634779\n",
      "Iteration 27, loss = 0.10604689\n",
      "Iteration 28, loss = 0.10573479\n",
      "Iteration 29, loss = 0.10540350\n",
      "Iteration 30, loss = 0.10513545\n",
      "Iteration 31, loss = 0.10482147\n",
      "Iteration 32, loss = 0.10453071\n",
      "Iteration 33, loss = 0.10422788\n",
      "Iteration 34, loss = 0.10392546\n",
      "Iteration 35, loss = 0.10366440\n",
      "Iteration 36, loss = 0.10333228\n",
      "Iteration 37, loss = 0.10306479\n",
      "Iteration 38, loss = 0.10269362\n",
      "Iteration 39, loss = 0.10238437\n",
      "Iteration 40, loss = 0.10210277\n",
      "Iteration 41, loss = 0.10176120\n",
      "Iteration 42, loss = 0.10141268\n",
      "Iteration 43, loss = 0.10106035\n",
      "Iteration 44, loss = 0.10074649\n",
      "Iteration 45, loss = 0.10043340\n",
      "Iteration 46, loss = 0.10009846\n",
      "Iteration 47, loss = 0.09976666\n",
      "Iteration 48, loss = 0.09945280\n",
      "Iteration 49, loss = 0.09913287\n",
      "Iteration 50, loss = 0.09880516\n",
      "Iteration 51, loss = 0.09850630\n",
      "Iteration 52, loss = 0.09815740\n",
      "Iteration 53, loss = 0.09781487\n",
      "Iteration 54, loss = 0.09750694\n",
      "Iteration 55, loss = 0.09717133\n",
      "Iteration 56, loss = 0.09683116\n",
      "Iteration 57, loss = 0.09652597\n",
      "Iteration 58, loss = 0.09621169\n",
      "Iteration 59, loss = 0.09589041\n",
      "Iteration 60, loss = 0.09556535\n",
      "Iteration 61, loss = 0.09528520\n",
      "Iteration 62, loss = 0.09494469\n",
      "Iteration 63, loss = 0.09466164\n",
      "Iteration 64, loss = 0.09438565\n",
      "Iteration 65, loss = 0.09409223\n",
      "Iteration 66, loss = 0.09375104\n",
      "Iteration 67, loss = 0.09336102\n",
      "Iteration 68, loss = 0.09309509\n",
      "Iteration 69, loss = 0.09267617\n",
      "Iteration 70, loss = 0.09227689\n",
      "Iteration 71, loss = 0.09193490\n",
      "Iteration 72, loss = 0.09160146\n",
      "Iteration 73, loss = 0.09123022\n",
      "Iteration 74, loss = 0.09085571\n",
      "Iteration 75, loss = 0.09055292\n",
      "Iteration 76, loss = 0.09008224\n",
      "Iteration 77, loss = 0.08975565\n",
      "Iteration 78, loss = 0.08940441\n",
      "Iteration 79, loss = 0.08902352\n",
      "Iteration 80, loss = 0.08851991\n",
      "Iteration 81, loss = 0.08810103\n",
      "Iteration 82, loss = 0.08769791\n",
      "Iteration 83, loss = 0.08721227\n",
      "Iteration 84, loss = 0.08675862\n",
      "Iteration 85, loss = 0.08624757\n",
      "Iteration 86, loss = 0.08580005\n",
      "Iteration 87, loss = 0.08523382\n",
      "Iteration 88, loss = 0.08459293\n",
      "Iteration 89, loss = 0.08397198\n",
      "Iteration 90, loss = 0.08337070\n",
      "Iteration 91, loss = 0.08274923\n",
      "Iteration 92, loss = 0.08214545\n",
      "Iteration 93, loss = 0.08156967\n",
      "Iteration 94, loss = 0.08094436\n",
      "Iteration 95, loss = 0.08043582\n",
      "Iteration 96, loss = 0.07980229\n",
      "Iteration 97, loss = 0.07910995\n",
      "Iteration 98, loss = 0.07851663\n",
      "Iteration 99, loss = 0.07780454\n",
      "Iteration 100, loss = 0.07702299\n",
      "Iteration 101, loss = 0.07620904\n",
      "Iteration 102, loss = 0.07537948\n",
      "Iteration 103, loss = 0.07458407\n",
      "Iteration 104, loss = 0.07387225\n",
      "Iteration 105, loss = 0.07305597\n",
      "Iteration 106, loss = 0.07232606\n",
      "Iteration 107, loss = 0.07170583\n",
      "Iteration 108, loss = 0.07102124\n",
      "Iteration 109, loss = 0.07037451\n",
      "Iteration 110, loss = 0.06967989\n",
      "Iteration 111, loss = 0.06897279\n",
      "Iteration 112, loss = 0.06828221\n",
      "Iteration 113, loss = 0.06753235\n",
      "Iteration 114, loss = 0.06678893\n",
      "Iteration 115, loss = 0.06590971\n",
      "Iteration 116, loss = 0.06508979\n",
      "Iteration 117, loss = 0.06409299\n",
      "Iteration 118, loss = 0.06329760\n",
      "Iteration 119, loss = 0.06215454\n",
      "Iteration 120, loss = 0.06048524\n",
      "Iteration 121, loss = 0.05765811\n",
      "Iteration 122, loss = 0.05470038\n",
      "Iteration 123, loss = 0.05170978\n",
      "Iteration 124, loss = 0.04880835\n",
      "Iteration 125, loss = 0.04668214\n",
      "Iteration 126, loss = 0.04458151\n",
      "Iteration 127, loss = 0.04260491\n",
      "Iteration 128, loss = 0.04056608\n",
      "Iteration 129, loss = 0.03849976\n",
      "Iteration 130, loss = 0.03647009\n",
      "Iteration 131, loss = 0.03461240\n",
      "Iteration 132, loss = 0.03276492\n",
      "Iteration 133, loss = 0.03118269\n",
      "Iteration 134, loss = 0.02940674\n",
      "Iteration 135, loss = 0.02776041\n",
      "Iteration 136, loss = 0.02610506\n",
      "Iteration 137, loss = 0.02462122\n",
      "Iteration 138, loss = 0.02307681\n",
      "Iteration 139, loss = 0.02169322\n",
      "Iteration 140, loss = 0.02018935\n",
      "Iteration 141, loss = 0.01888391\n",
      "Iteration 142, loss = 0.01763892\n",
      "Iteration 143, loss = 0.01632703\n",
      "Iteration 144, loss = 0.01522490\n",
      "Iteration 145, loss = 0.01409034\n",
      "Iteration 146, loss = 0.01310578\n",
      "Iteration 147, loss = 0.01220146\n",
      "Iteration 148, loss = 0.01139513\n",
      "Iteration 149, loss = 0.01064282\n",
      "Iteration 150, loss = 0.01001354\n",
      "Iteration 151, loss = 0.00938253\n",
      "Iteration 152, loss = 0.00886294\n",
      "Iteration 153, loss = 0.00838430\n",
      "Iteration 154, loss = 0.00790098\n",
      "Iteration 155, loss = 0.00749247\n",
      "Iteration 156, loss = 0.00712594\n",
      "Iteration 157, loss = 0.00680117\n",
      "Iteration 158, loss = 0.00650500\n",
      "Iteration 159, loss = 0.00624389\n",
      "Iteration 160, loss = 0.00600038\n",
      "Iteration 161, loss = 0.00579024\n",
      "Iteration 162, loss = 0.00558642\n",
      "Iteration 163, loss = 0.00540812\n",
      "Iteration 164, loss = 0.00526576\n",
      "Iteration 165, loss = 0.00512415\n",
      "Iteration 166, loss = 0.00501229\n",
      "Iteration 167, loss = 0.00489161\n",
      "Iteration 168, loss = 0.00478498\n",
      "Iteration 169, loss = 0.00471132\n",
      "Iteration 170, loss = 0.00466007\n",
      "Iteration 171, loss = 0.00450897\n",
      "Iteration 172, loss = 0.00447508\n",
      "Iteration 173, loss = 0.00439251\n",
      "Iteration 174, loss = 0.00433869\n",
      "Iteration 175, loss = 0.00426687\n",
      "Iteration 176, loss = 0.00421540\n",
      "Iteration 177, loss = 0.00416481\n",
      "Iteration 178, loss = 0.00412076\n",
      "Iteration 179, loss = 0.00410964\n",
      "Iteration 180, loss = 0.00404491\n",
      "Iteration 181, loss = 0.00400998\n",
      "Iteration 182, loss = 0.00397620\n",
      "Iteration 183, loss = 0.00396282\n",
      "Iteration 184, loss = 0.00391802\n",
      "Iteration 185, loss = 0.00393108\n",
      "Iteration 186, loss = 0.00387326\n",
      "Iteration 187, loss = 0.00388536\n",
      "Iteration 188, loss = 0.00382364\n",
      "Iteration 189, loss = 0.00381995\n",
      "Iteration 190, loss = 0.00380029\n",
      "Iteration 191, loss = 0.00378711\n",
      "Iteration 192, loss = 0.00376303\n",
      "Iteration 193, loss = 0.00373732\n",
      "Iteration 194, loss = 0.00371771\n",
      "Iteration 195, loss = 0.00369926\n",
      "Iteration 196, loss = 0.00368620\n",
      "Iteration 197, loss = 0.00367556\n",
      "Iteration 198, loss = 0.00366584\n",
      "Iteration 199, loss = 0.00365101\n",
      "Iteration 200, loss = 0.00363240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GAMER\\anaconda3\\envs\\dataAnalyst\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(hidden_layer_sizes=[16, 8, 1], learning_rate_init=0.01,\n",
       "             n_iter_no_change=1000, solver='sgd', verbose=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn as sk\n",
    "import sklearn.neural_network\n",
    "\n",
    "lr = 0.01\n",
    "nn=[2,16,8,1]\n",
    "\n",
    "# Existen tipos de modelos como MLPRegressor, BernoulliRBM MLPClassifier\n",
    "# multilayer_perceptron y rbm\n",
    "model = sk.neural_network.MLPRegressor(solver='sgd',\n",
    "                                        learning_rate_init = lr,\n",
    "                                        hidden_layer_sizes=nn[1:],\n",
    "                                        verbose = True,\n",
    "                                       n_iter_no_change = 1000)\n",
    "\n",
    "model.fit(X,y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
